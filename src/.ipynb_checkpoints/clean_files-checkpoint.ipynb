{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project in Data Intensive Computing\n",
    "Authors: Alex Hermansson and Elin Samuelsson\n",
    "\n",
    "## Blabla Political Parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (2.3.2)\n",
      "Requirement already satisfied: py4j==0.10.7 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pyspark) (0.10.7)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "In this cell, we simply initiliaze the sparkSession and create some useful variables such as the paths to the json files containing votes (and other metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DataIntensive project\").getOrCreate()\n",
    "\n",
    "top_dir = \"../data\"\n",
    "paths = [os.path.join(top_dir, path)\n",
    "         for path in os.listdir(top_dir) \n",
    "         if path.endswith(\".json\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "In the cell below, we clean the json files (they are on some sort of weird format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_json(path_to_file):\n",
    "    f = open(path_to_file, \"r\")\n",
    "    lines = f.readlines()\n",
    "    length = len(lines)\n",
    "    f.close()\n",
    "\n",
    "    all_lines = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i not in [0, 1, 2, length-1, length-2, length-3, length-4]:\n",
    "            all_lines.append(line)\n",
    "\n",
    "    lines_str = ''.join(all_lines)\n",
    "    lines = lines_str.split(\"},\")\n",
    "    lines = ''.join([line + \"}\" for line in lines])\n",
    "    lines = lines.replace('\\n', '')\n",
    "    lines = lines.replace('}', '}\\n')\n",
    "    lines = lines.replace('  ', '')\n",
    "    lines = lines.replace(',', ', ')\n",
    "    \n",
    "    ## Move this into a map function?\n",
    "    lines = lines.replace('Ja', '1')\n",
    "    lines = lines.replace('Nej', '-1')\n",
    "    lines = lines.replace('Avstår', '0')\n",
    "    lines = lines.replace('Frånvarande', '0')\n",
    "\n",
    "    with open(path_to_file, \"w\") as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "            \n",
    "                        \n",
    "def clean_data():\n",
    "    for path in paths:\n",
    "        clean_json(path)\n",
    "    \n",
    "    # Removing paths with more than one voting round\n",
    "    popped_counter = 0\n",
    "    for i in range(len(paths)):\n",
    "        with open(paths[i - popped_counter]) as f:\n",
    "            lines = f.readlines()\n",
    "            l = len(lines)\n",
    "            if l != 349:\n",
    "                paths.pop(i - popped_counter)\n",
    "                popped_counter += 1\n",
    "                \n",
    "## Only run the following function if the data needs cleaning.. if its already \"clean\", it messes it up completely\n",
    "clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Information\n",
    "\n",
    "Here we store the information that is static with respect to different voting rounds. It includes names, political parties etc. Also, we map the Swedish names to English ones and we modify one column to store age instead of year of birth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+---+\n",
      "|                name|party|   sex|        constituency|age|\n",
      "+--------------------+-----+------+--------------------+---+\n",
      "|      Andreas Norlén|    M|   man|   Östergötlands län| 45|\n",
      "|     Ulrika Carlsson|    C|kvinna|Västra Götalands ...| 53|\n",
      "| Margareta Cederfelt|    M|kvinna|   Stockholms kommun| 59|\n",
      "|   Christina Östberg|   SD|kvinna|          Kalmar län| 50|\n",
      "|   Cecilia Magnusson|    M|kvinna|    Göteborgs kommun| 56|\n",
      "|     Penilla Gunther|   KD|kvinna|Västra Götalands ...| 54|\n",
      "|      Jonas Eriksson|   MP|   man|          Örebro län| 51|\n",
      "|          Per Åsling|    C|   man|       Jämtlands län| 61|\n",
      "|      Peter Jeppsson|    S|   man|        Blekinge län| 50|\n",
      "|         Lawen Redar|    S|kvinna|   Stockholms kommun| 29|\n",
      "|      1n R Andersson|    M|   man|          Kalmar län| 48|\n",
      "|    Robert Stenkvist|   SD|   man|Västra Götalands ...| 60|\n",
      "|      Johan Forssell|    M|   man|   Stockholms kommun| 39|\n",
      "|Annika Hirvonen Falk|   MP|kvinna|      Stockholms län| 29|\n",
      "|       Sara Karlsson|    S|kvinna|   Södermanlands län| 33|\n",
      "|     Andreas Carlson|   KD|   man|      Jönköpings län| 31|\n",
      "|       Aron Emilsson|   SD|   man|   Skåne läns västra| 28|\n",
      "|      Finn Bengtsson|    M|   man|   Östergötlands län| 62|\n",
      "|      Sten Bergheden|    M|   man|Västra Götalands ...| 53|\n",
      "|     Désirée Pethrus|   KD|kvinna|   Stockholms kommun| 59|\n",
      "+--------------------+-----+------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ = spark.read.json(os.path.join(top_dir, paths[0]))\n",
    "df_info = df_.select(df_[\"namn\"].alias(\"name\"), \n",
    "                     df_[\"parti\"].alias(\"party\"),\n",
    "                     df_[\"kon\"].alias(\"sex\"),\n",
    "                     df_[\"valkrets\"].alias(\"constituency\"),\n",
    "                     (2018 - df_[\"fodd\"]).alias(\"age\").cast(\"int\")\n",
    "                    )\n",
    "df_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features\n",
    "\n",
    "Below, we merge the votes for different rounds into our final DataFrame. Here we have all the \"static\" information for each parliment member, and also their votes.\n",
    "We have chosen to map the votes as the following:\n",
    "- \"Yes\" to 1, \n",
    "- \"No\" to -1, \n",
    "- \"Refrain\" to 0, \n",
    "- \"Absent\" to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vote_to_int = {\"Ja\": 1, \"Nej\": 0, \"Avstår\": -1, \"Frånvarande\": -2}\n",
    "# .rdd.map(lambda vote: vote_to_int[vote])\n",
    "questions = 10\n",
    "df = df_info\n",
    "for question_number, path in enumerate(paths[:questions], 1):\n",
    "    column_name = \"q%s\" % question_number\n",
    "    df_i = spark.read.json(os.path.join(top_dir, path))\n",
    "    df_vote = df_i.select(df_i[\"namn\"].alias(\"name\"), df_i[\"rost\"].alias(column_name))\n",
    "    df = df.join(df_vote, \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|                name|party|   sex|        constituency|age| q1| q2| q3| q4| q5| q6| q7| q8| q9|q10|\n",
      "+--------------------+-----+------+--------------------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|      Andreas Norlén|    M|   man|   Östergötlands län| 45|  0| -1|  1|  1| -1|  1| -1|  1|  0|  1|\n",
      "|     Ulrika Carlsson|    C|kvinna|Västra Götalands ...| 53|  0| -1|  0|  1|  1|  1|  0| -1|  1|  1|\n",
      "| Margareta Cederfelt|    M|kvinna|   Stockholms kommun| 59|  0| -1|  0|  1|  0|  1| -1|  1|  1|  0|\n",
      "|   Christina Östberg|   SD|kvinna|          Kalmar län| 50| -1|  1| -1| -1|  1|  1|  1|  1|  1| -1|\n",
      "|   Cecilia Magnusson|    M|kvinna|    Göteborgs kommun| 56|  0| -1|  1|  1| -1|  1| -1|  1|  1|  1|\n",
      "|     Penilla Gunther|   KD|kvinna|Västra Götalands ...| 54|  0| -1|  0|  0|  1| -1| -1|  1|  0|  1|\n",
      "|      Jonas Eriksson|   MP|   man|          Örebro län| 51|  1|  1|  1|  0|  0|  1|  1|  1|  1|  1|\n",
      "|          Per Åsling|    C|   man|       Jämtlands län| 61|  0| -1|  0|  1|  1|  1| -1|  0|  1|  1|\n",
      "|      Peter Jeppsson|    S|   man|        Blekinge län| 50|  1|  1|  1|  1|  1|  1|  1|  1|  0|  0|\n",
      "|         Lawen Redar|    S|kvinna|   Stockholms kommun| 29|  1|  1|  1|  1|  1|  1|  1|  1|  1|  0|\n",
      "|      1n R Andersson|    M|   man|          Kalmar län| 48|  0| -1|  1|  1| -1|  1| -1|  1|  1|  1|\n",
      "|    Robert Stenkvist|   SD|   man|Västra Götalands ...| 60| -1|  1| -1| -1|  1|  1|  1|  1|  1| -1|\n",
      "|      Johan Forssell|    M|   man|   Stockholms kommun| 39|  0| -1|  1|  1| -1|  0| -1|  1|  1|  1|\n",
      "|Annika Hirvonen Falk|   MP|kvinna|      Stockholms län| 29|  1|  1|  1|  1|  1|  0|  1|  1|  1|  1|\n",
      "|       Sara Karlsson|    S|kvinna|   Södermanlands län| 33|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|     Andreas Carlson|   KD|   man|      Jönköpings län| 31|  0| -1|  0|  1|  1| -1| -1|  1| -1|  1|\n",
      "|       Aron Emilsson|   SD|   man|   Skåne läns västra| 28| -1|  1| -1| -1|  1|  1|  1|  1|  1| -1|\n",
      "|      Finn Bengtsson|    M|   man|   Östergötlands län| 62|  0| -1|  1|  1| -1|  1|  0|  1|  1|  1|\n",
      "|      Sten Bergheden|    M|   man|Västra Götalands ...| 53|  0| -1|  1|  1| -1|  0| -1|  1|  1|  1|\n",
      "|     Désirée Pethrus|   KD|kvinna|   Stockholms kommun| 59|  0| -1|  0|  1|  1| -1| -1|  1| -1|  1|\n",
      "+--------------------+-----+------+--------------------+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 -1  1 ...,  1  0  1]\n",
      " [ 0 -1  0 ..., -1  1  1]\n",
      " [ 0 -1  0 ...,  1  1  0]\n",
      " ..., \n",
      " [ 1  1  1 ...,  1  0  1]\n",
      " [-1  1 -1 ...,  0  1 -1]\n",
      " [ 1  1  1 ...,  1  1  1]]\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "| q1| q2| q3| q4| q5| q6| q7| q8| q9|q10|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0| -1|  1|  1| -1|  1| -1|  1|  0|  1|\n",
      "|  0| -1|  0|  1|  1|  1|  0| -1|  1|  1|\n",
      "|  0| -1|  0|  1|  0|  1| -1|  1|  1|  0|\n",
      "| -1|  1| -1| -1|  1|  1|  1|  1|  1| -1|\n",
      "|  0| -1|  1|  1| -1|  1| -1|  1|  1|  1|\n",
      "|  0| -1|  0|  0|  1| -1| -1|  1|  0|  1|\n",
      "|  1|  1|  1|  0|  0|  1|  1|  1|  1|  1|\n",
      "|  0| -1|  0|  1|  1|  1| -1|  0|  1|  1|\n",
      "|  1|  1|  1|  1|  1|  1|  1|  1|  0|  0|\n",
      "|  1|  1|  1|  1|  1|  1|  1|  1|  1|  0|\n",
      "|  0| -1|  1|  1| -1|  1| -1|  1|  1|  1|\n",
      "| -1|  1| -1| -1|  1|  1|  1|  1|  1| -1|\n",
      "|  0| -1|  1|  1| -1|  0| -1|  1|  1|  1|\n",
      "|  1|  1|  1|  1|  1|  0|  1|  1|  1|  1|\n",
      "|  1|  1|  1|  1|  1|  1|  1|  1|  1|  1|\n",
      "|  0| -1|  0|  1|  1| -1| -1|  1| -1|  1|\n",
      "| -1|  1| -1| -1|  1|  1|  1|  1|  1| -1|\n",
      "|  0| -1|  1|  1| -1|  1|  0|  1|  1|  1|\n",
      "|  0| -1|  1|  1| -1|  0| -1|  1|  1|  1|\n",
      "|  0| -1|  0|  1|  1| -1| -1|  1| -1|  1|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1369.numRows.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 469.0 failed 1 times, most recent failure: Lost task 0.0 in stage 469.0 (TID 469, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 181, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1837)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numRows(RowMatrix.scala:75)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 181, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1837)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-a8ad4e269b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRowMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36mnumRows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_matrix_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numRows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1369.numRows.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 469.0 failed 1 times, most recent failure: Lost task 0.0 in stage 469.0 (TID 469, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 181, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1837)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numRows(RowMatrix.scala:75)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 181, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1837)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vector\n",
    "import numpy as np\n",
    "\n",
    "features = ['q%i' % (i+1) for i in range(questions)]\n",
    "df_features = df.select(features)\n",
    "\n",
    "a = np.array(df_features.collect(), dtype=int)\n",
    "print(a)\n",
    "\n",
    "# rdd_vec = df_features.rdd.map(lambda row: Vector(np.array(row, dtype=int)))\n",
    "rdd_vec = df_features.rdd.map(lambda row: Vector(row))\n",
    "df_features.show()\n",
    "\n",
    "matrix = RowMatrix(rdd_vec)\n",
    "print(matrix.numRows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
